{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('only_translated.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408602,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data['Translated'].dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words(x, terms = 30):\n",
    "  all_words = ' '.join([text for text in x])\n",
    "  all_words = all_words.split()\n",
    "  \n",
    "  fdist = FreqDist(all_words)\n",
    "  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n",
    "  \n",
    "  # selecting top 20 most frequent words\n",
    "  d = words_df.nlargest(columns=\"count\", n = terms) \n",
    "  plt.figure(figsize=(20,5))\n",
    "  ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n",
    "  ax.set(ylabel = 'Count')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marvin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopwords(rev):\n",
    "  rev_new = \" \".join([i for i in rev if i not in stop_words])\n",
    "  return rev_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short words (length < 3)\n",
    "data = data.apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "# remove stopwords from the text\n",
    "data = [remove_stopwords(r.split()) for r in data]\n",
    "\n",
    "# make entire text lowercase\n",
    "data = [r.lower() for r in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "def lemmatization(texts, tags=['NOUN', 'ADJ']):\n",
    "    output = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        output.append([token.lemma_ for token in doc if token.pos_ in tags])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'comments', 'used?', 'can', 'still', 'used', 'before,', 'obviously', 'now?']\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = pd.Series(data).apply(lambda x: x.split())\n",
    "print(tokenized_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comment']\n"
     ]
    }
   ],
   "source": [
    "data_2 = lemmatization(tokenized_data)\n",
    "print(data_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(data_2)):\n",
    "    t = ' '.join(data_2[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "data_2 = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408602, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features= 1000, # keep top 1000 terms \n",
    "                             max_df = 0.5, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(data_2)\n",
    "\n",
    "X.shape # check shape of the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "good\n",
      "app\n",
      "application\n",
      "map\n",
      "navigation\n",
      "great\n",
      "gps\n",
      " \n",
      "Topic 1: \n",
      "app\n",
      "great\n",
      "nice\n",
      "excellent\n",
      "useful\n",
      "map\n",
      "love\n",
      " \n",
      "Topic 2: \n",
      "nice\n",
      "good\n",
      "aap\n",
      "working\n",
      "root\n",
      "graphic\n",
      "updating\n",
      " \n",
      "Topic 3: \n",
      "excellent\n",
      "application\n",
      "nice\n",
      "useful\n",
      "map\n",
      "thank\n",
      "time\n",
      " \n",
      "Topic 4: \n",
      "great\n",
      "nice\n",
      "excellent\n",
      "application\n",
      "good\n",
      "help\n",
      "program\n",
      " \n",
      "Topic 5: \n",
      "helpful\n",
      "map\n",
      "application\n",
      "useful\n",
      "thank\n",
      "easy\n",
      "accurate\n",
      " \n",
      "Topic 6: \n",
      "useful\n",
      "application\n",
      "map\n",
      "use\n",
      "time\n",
      "easy\n",
      "route\n",
      " \n",
      "Topic 7: \n",
      "map\n",
      "application\n",
      "update\n",
      "time\n",
      "love\n",
      "route\n",
      "use\n",
      " \n",
      "Topic 8: \n",
      "application\n",
      "super\n",
      "love\n",
      "app\n",
      "time\n",
      "gps\n",
      "update\n",
      " \n",
      "Topic 9: \n",
      "love\n",
      "waze\n",
      "use\n",
      "easy\n",
      "time\n",
      "traffic\n",
      "accurate\n",
      " \n",
      "Topic 10: \n",
      "super\n",
      "love\n",
      "useful\n",
      "excellent\n",
      "great\n",
      "navigation\n",
      "easy\n",
      " \n",
      "Topic 11: \n",
      "time\n",
      "use\n",
      "update\n",
      "bad\n",
      "easy\n",
      "route\n",
      "gps\n",
      " \n",
      "Topic 12: \n",
      "awesome\n",
      "easy\n",
      "use\n",
      "thank\n",
      "accurate\n",
      "super\n",
      "great\n",
      " \n",
      "Topic 13: \n",
      "use\n",
      "easy\n",
      "comment\n",
      "accurate\n",
      "application\n",
      "map\n",
      "app\n",
      " \n",
      "Topic 14: \n",
      "bad\n",
      "easy\n",
      "use\n",
      "map\n",
      "awesome\n",
      "love\n",
      "application\n",
      " \n",
      "Topic 15: \n",
      "cool\n",
      "bad\n",
      "thank\n",
      "program\n",
      "thing\n",
      "bro\n",
      "lot\n",
      " \n",
      "Topic 16: \n",
      "thank\n",
      "bad\n",
      "lot\n",
      "time\n",
      "route\n",
      "road\n",
      "traffic\n",
      " \n",
      "Topic 17: \n",
      "update\n",
      "gps\n",
      "thank\n",
      "location\n",
      "signal\n",
      "work\n",
      "new\n",
      " \n",
      "Topic 18: \n",
      "gps\n",
      "accurate\n",
      "bad\n",
      "signal\n",
      "location\n",
      "thank\n",
      "time\n",
      " \n",
      "Topic 19: \n",
      "accurate\n",
      "location\n",
      "navigation\n",
      "update\n",
      "steady\n",
      "point\n",
      "direction\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f50547fd9984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "X_topics = svd_model.fit_transform(X)\n",
    "embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "c = dataset.target,\n",
    "s = 10, # size\n",
    "edgecolor='none'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408602, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get document_topic_matrix with SVD\n",
    "document_topic_matrix = svd_model.transform(X)\n",
    "\n",
    "document_topic_matrix.shape #check shape of the document-topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408648, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
